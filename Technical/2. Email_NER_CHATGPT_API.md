# Email NER + ChatGPT API


NER stands for Named Entity Recognition in Natural Language Processing (NLP). It is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.

For example, in the sentence "Apple is headquartered in Cupertino, California," an NER system would identify "Apple" as an organization and "Cupertino, California" as a location.

The provided text seems to be a shipping request of the company. We usually receive these emails in the format of .eml file We perform NER on the email body and the subject.

We received the dataset from the clients usually in the form of .eml files. The `.eml` file format is used to store email messages. Each `.eml` file represents a single email message, including its content, attachments, and metadata. The term "EML" stands for "Electronic Mail Message."

**Named Entity Recognition (NER) Entities:**
- Client_reference
- Pick_up_date
- Pick_up_province
- Delivery_address
- Delivery_city
- Delivery_zipcode
- Delivery_name

![Alt text](<Images/Screenshot 2024-01-22 at 00.37.29.png>)

The annotation of these datasets is performed using:

- Label Studio  - Label Studio is an open-source data labeling tool used for machine learning projects. It provides a user-friendly interface for annotating various types of data, such as text, images, audio, and video, to create labeled datasets for training machine learning models.
  
- Auto Annotate the small subset of the model.
  
- We also make use of ChatGPT API service turbo 3.5 to annotate the dataset because it takes a lot of time.

We convert these `.eml` files to proper text and transform the dataset by words and labels.

**Preprocessing:**
- Removing special characters
- Remove URL
- Remove phone numbers & websites
- Remove lines with more than 4 consecutive characters '-'
- Custom rules for domain-specific entities
- Handling email signatures and metadata
- Removal of phrases like "Thank and regards" that remove all the information
- Annotate the remaining entities as others


**The three steps in named entity recognition are:**

1. Tokenization, which involves breaking the text into individual words or phrases.
2. Part-of-speech tagging, which assigns a grammatical tag to each word.
3. Entity recognition, which identifies and classifies the named entities in the text.


**What is a SpaCy NER?**

SpaCy is an open-source natural language processing (NLP) library written in Python that is designed to be fast, efficient, and production-ready. One of its key functionalities is Named Entity Recognition (NER), which is the process of identifying and classifying named entities (such as names of people, organizations, locations, etc.) within a text.

Customizing SpaCy's NER model allows you to train the model on your own dataset to recognize specific named entities relevant to your domain or task. Here's a step-by-step guide to creating a custom NER model with SpaCy:

**BERT Model:**
- BERT (Bidirectional Encoder Representations from Transformers)
  - Developed by Google in 2018
  - Trained on Wikipedia + BookCorpus (3500 million words)
  - Uses masked language modeling and next sentence prediction during pre-training
  - Strong performance on a wide range of NLP tasks after fine-tuning

**RoBERTa Model:**
- RoBERTa (A Robustly Optimized BERT Pretraining Approach)
  - An improvement on BERT from Facebook AI Research in 2019
  - Trained on more data (160GB of text)
  - Removes next sentence prediction, trains longer, and dynamically masks words
  - Outperforms BERT in several benchmarks

**XLM-RoBERTa Model:**
- XLM-RoBERTa (Cross-lingual Language Model Pretraining)
  - An adaptation of RoBERTa from Facebook in 2019
  - Trained on 2.5TB of filtered CommonCrawl data in 100 languages
  - Adds translational language modeling objective during pre-training
  - State-of-the-art cross-lingual understanding without any modification

In summary, RoBERTa improved on BERT's pretraining, while XLM-RoBERTa extended RoBERTa to handle multiple languages. Both models demonstrate superior performance over BERT in several NLP tasks. Out of the three, XLM-RoBERTa performed the best, achieving an accuracy of 76%. We have annotated approximately 600 emails.

**ChatGPT Dataset Preparation + Total Weight Quantity:**

We also make use of ChatGPT API service turbo 3.5 to annotate the dataset by developing prompts. We tested with different prompts because it takes a lot of time.
- Reduced development time
- Consistency in annotations

We developed the prompt by testing out different prompts.

![Alt text](<Images/Screenshot 2024-01-22 at 01.22.51.png>)

**Total Weight Quantity:**

Letâ€™s consider an example email where there is no mention of the weight in the email but it is written like 3 pallets of 100 kg each. So in the case a normal BERT/Roberta /XLM Roberta Model would just give the prediction as 100 kg as weight but the actual answer would be if a human reads it that 3*100 kg = 300 kg

In some cases we pass the whole email body as a text and just to extract one entity from ChatGPT API like the total weight.

**A pallet is a flat, portable platform with both top and bottom deck boards, designed for storing, handling, and transporting goods in a standardized and efficient manner.**
1 pallet

![Alt text](Images/15161817131885835782clipart-pallet.med.png)

**Richiesta nolo groupage Tuticorin / india**
Perfavore mi quotate da prato a; su TUTICORIN / India ::; - 3 pallets, ciascuno ca cm 115x135x220H e peso lordo tot dei 3 pallets of kg 1.360 each; Grazie e saluti; [cid:image001.png@01D9E00D.546B86F0]; Stefania Sardella
