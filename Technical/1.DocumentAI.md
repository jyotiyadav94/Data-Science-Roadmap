# Document Entity Extraction

## Use Case
Wenda is an organization that specializes in supply chain management. They have a platform for Automated data entry with AI. They want to automate the reading of documents from any unstructured documents to improve their operations. A lot of people still perform manual data entry jobs. This process is often time consuming and error-prone. For example manually entering data from invoices & packing lists can be a very tedious task.

I have worked on this project from
- **Model Literature Overview:** Defining the problem/objective of the project, thorough existing knowledge related to your project. Studying research papers, articles, state-of-the-art technologies, methodologies & best practices
- **Data collection & Preprocessing:** Gathering the dataset like the PDF’s
- **Model Selection & Development**
- **Model Evaluation**
- **Model Deployment**
- **Monitoring & Maintenance**

## Data Categories
- **Bill of Lading Instructions:** Instructions provided by the shipper to the carrier detailing the terms and conditions for transporting goods.
- **Invoices:** Itemized documents specifying the goods sold, their quantities, and prices, serving as a billing statement from the seller to the buyer.
- **Packing List:** Detailed inventory of the contents, quantities, and packaging of shipped goods, assisting in cargo inspection and customs clearance.
- **Certificates:** Various documents, such as a Certificate of Origin, certifying specific details about the goods or their compliance with standards.
- **Insurance Certificate:** Document evidencing insurance coverage for shipped goods, protecting against loss or damage during transit.
- **Customs Declaration:** Form detailing information about imported or exported goods submitted to customs authorities for clearance.
- **Airway Bill (AWB):** Document used in air transport acknowledging the receipt of goods by an airline carrier, serving as a receipt and a contract for carriage.

## Document AI Transformer Models
We make use of document AI transformer models to improve supply chain automation efficiency & reduce document processing time to a larger extent.


![Alt text](<Images/Screenshot 2024-01-21 at 19.43.40.png>)

**Different data structure inputs:**
All the datasets received have been transformed into the Images using open source libraries:
- PDF
- Word
- .msg (extract_msg)
- Excel
- Scanned Images

## Custom Dataset Preparation
**Annotation method using:**
- Vgg Annotation tool
- VGG Image Annotator is a simple and standalone manual annotation software for image, audio, and video. VIA runs in a web browser and does not require any installation or setup. The complete VIA software fits on a single self-contained HTML page of size less than 400 Kilobyte that runs as an offline application in most modern web browsers.
- Label Studio

We have automated this task by performing auto-labeling - We create a model with a very small subset of the dataset. Use the same model to auto-annotate the labels and correct a few of them in case of wrong predictions. This has reduced our time in the preparation of the dataset where we have a lot of datasets because we have a small team to annotate the datasets. We deal with annotating dataset in 500 - 700 PDF documents.

We prepare this dataset and then feed it to the OCR which actually extracts the information related to bounding boxes (xmin, xmax, ymin, ymax), text, and confidence score. We have annotated the dataset from the annotation tool. We have created this extra column “label” called which contains all the text as “others” and it pickles the annotated dataset from the file.

The dataset is a combination of Images + text + Layout information

## Model
### Dataset Information
It is a multi-modal. It uses the combination of text + layout + Image features.

Here are a few key things to know about LayoutLM models:

### LayoutLM
LayoutLM is a transformer-based model architecture designed for document AI tasks like form understanding, receipt understanding, document image classification etc. Microsoft in 2020.

LayoutLM to jointly model interactions between text & layout information across scanned document images. It uses 2D positional embeddings (xmin, xmax, ymin, ymax) and text embedding these five embeddings to train the layout LM model. It uses the concept of Masked language Modeling (MLM) - certain words or tokens in a sentence are randomly masked, and the model is trained to predict these masked words based on the context provided by the surrounding words. MLM only for text embedding But position embeddings are not masked.

![Alt text](<Images/Screenshot 2024-01-21 at 20.31.31.png>)

Then they also added an extra layer of information Faster R CNN and also added these image embeddings which have been used for downstream tasks.

This approach combines text and image features in a document by using an image embedding layer. They split the document image into pieces, aligning each piece with the words via OCR bounding boxes. Features are generated for these pieces using Faster R-CNN, serving as token image embeddings. For the special token [CLS], representing the entire document, embeddings are created using the whole document image to aid downstream tasks requiring [CLS] token representation.

Compared to previous approaches, LayoutLM achieves much better performance on several document AI benchmark datasets like FUNDS, CORD, DocVQA etc. This is because it jointly models the visual layout and textual semantics on a document.

There are a few variants of LayoutLM like LayoutLMv2, LayoutLMv3, LayoutXLM which make improvements over the original model. These include better handling of multiple languages, integration of object detection capabilities, and ability to generalize across domains.

Using a pre-trained LayoutLM model and fine-tuning on custom labeled data is the typical way to apply these models to real-world document understanding problems like invoice processing, resume parsing, analyzing scientific papers etc.

LayoutLM models open up many possibilities for automating workflows involving analyzing documents images and extracting information from them. They eliminate the need for templates and rule-based extraction.

In summary, LayoutLM allows applying powerful NLP transformers like BERT to document images in an end-to-end fashion by encoding both layout and language information. Its variants and successors enable even stronger document understanding capabilities.

### Difference between LayoutLM & LayoutLMV3
3 objectives: masked language modeling (MLM), masked image modeling (MIM) and word-patch alignment (WPA).

![Alt text](<Images/Screenshot 2024-01-21 at 20.52.46.png>)

LayoutLM and LayoutLMv3 are two models developed by Microsoft for understanding document images and extracting information from them. The key differences are:
- **Architecture:** LayoutLM is based on BERT architecture, while LayoutLMv3 is based on Vision Transformers like ViT and DEiT. This makes LayoutLMv3 more suitable for computer vision tasks.
- **Pre-training data:** LayoutLMv3 is pre-trained on a much larger dataset called DocVQA, which has over 310k document images. LayoutLM was pre-trained on a smaller proprietary dataset.
- **Multimodal:** LayoutLMv3 is designed as a multi-modal model combining both vision and language understanding. Features from image patches and text tokens are integrated better compared to LayoutLM.
- **Performance:** LayoutLMv3 significantly outperforms LayoutLM and previous SOTA models on document image understanding tasks like document visual question answering, document classification, form understanding, receipt understanding etc.
- **Speed:** LayoutLMv3 has the capability to trade-off accuracy for speed. It is available in base, large and huge variants with increasing size and accuracy. In summary, LayoutLMv3 builds on LayoutLM with a multi-modal Transformer approach, pre-training on a larger dataset and achieving new state-of-the-art results on document intelligence tasks. The architecture upgrades make it more suited for vision-heavy tasks.

## OCR
- Google Vision OCR
- Amazon Textract
- Pytesseract
- Paddle OCR
- Easy OCR
- Keras OCR

## Image Processing
- Rotation of Images
- Skewness correction
- Remove borders
- Smoothening
- Noise reduction
- Converting Image to RGB

## Preprocessing
- Removing irrelevant characters like special characters,
- Remove any null or blanks
- Remove values or text with a confidence score of -1.

For the initial POC development of the model, we have:
- Hugging Face
- Gradio
- Weights & Biases
- Inference & Post-processing code
- Key value or the detail part which actually contains the table information
- Then we do the post-processing on the values if there are some extra special characters or some kind of random characters for the data validation.

## Evaluation Metrics

Evaluation metrics are used to assess the performance of a machine learning model by comparing its predictions to the actual outcomes. Different metrics provide insights into different aspects of a model's performance. Here's an explanation of the commonly used evaluation metrics:

### Precision:
Precision is a measure of the accuracy of the positive predictions made by a model. It is the ratio of true positive predictions to the total number of positive predictions (true positives + false positives). Formula: Precision = TP / (TP + FP) A high precision indicates that when the model predicts a positive outcome, it is likely to be correct.

### Recall (Sensitivity or True Positive Rate):
Recall measures the ability of a model to capture all the relevant instances. It is the ratio of true positive predictions to the total number of actual positive instances (true positives + false negatives). Formula: Recall = TP / (TP + FN) A high recall indicates that the model is good at finding all the positive instances.

### F1 Score:
F1 Score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is especially useful when there is an uneven class distribution. Formula: F1 Score = 2 * (Precision * Recall) / (Precision + Recall) F1 Score is a good metric when you want to consider both false positives and false negatives in the evaluation.

### Accuracy:
Accuracy is a simple and widely used metric that measures the overall correctness of the model's predictions. It is the ratio of correct predictions (true positives + true negatives) to the total number of predictions. Formula: Accuracy = (TP + TN) / (TP + TN + FP + FN) Accuracy is suitable for balanced datasets but may not be reliable when dealing with imbalanced datasets, where one class dominates the other.

It's essential to choose the evaluation metrics based on the specific goals and characteristics of the problem at hand. For example, in medical diagnosis, where false negatives can be critical, recall might be more important, while in spam detection, precision could be prioritized to minimize false positives.

![Alt text](<Images/confusion-matrix.webp>)

The matrix displays the number of instances produced by the model on the test data.

* True positives (TP): occur when the model accurately predicts a positive data point.
* True negatives (TN): occur when the model accurately predicts a negative data point.
* False positives (FP): occur when the model predicts a positive data point incorrectly.
* False negatives (FN): occur when the model mispredicts a negative data point.

### Main Abstract of the Research Paper

**LayoutLMv3** is an artificial intelligence model developed by Microsoft that specializes in understanding documents that contain both text and images, like forms, receipts, invoices, etc.

Proposed in the paper "LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking" (Huang et al., 2022). In this paper, we propose the **LayoutLMv3** to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. 

- Simplifies **LayoutLMv2** by using **ViT-style patch embeddings** instead of a CNN backbone

- Pre-trained on 3 objectives:

    1. **Masked Language Modelling (MLM)** on text:
        - This technique masks or hides random words in the text of the input document image. The model is then trained to predict the original masked words based on the surrounding text. Doing this allows the model to learn robust language representations and dependencies between words in document text.

    2. **Masked Image Modelling (MIM)** on images:
        - Similar to MLM, this technique masks random image patches in the input document image. The model has to predict what the masked image patches contained originally. This helps the model understand visual features and layout patterns in documents, like logos, signatures, tables without relying just on text.

    3. **Word-Patch Alignment (WPA)** to align text and image patches:
        - This objective trains the model to predict if an image patch was masked given a masked text token and vice versa. For example, if the word "Total" in the text was masked, the model learns to recognize if the corresponding numeric "Total" cell in the image is visible or masked. This aligns the text tokens and image patches. Doing this enables **LayoutLMv3** to deeply understand the connection between document text and layout. So in summary, the unified masking approach over text and images enables robust understanding of documents involving both modalities.

- Unified architecture and pre-training help it be a general-purpose model for both text-centric and image-centric document AI tasks.

- Achieves state-of-the-art results on form understanding, receipt understanding, document visual QA, document image classification, and document layout analysis benchmarks.

- Main innovation is the unified masking scheme and objectives over text and images during pre-training

**LayoutLMv3** architecture. Taken from the original paper.

### How It Works

**LayoutLMv3** builds on previous work called **LayoutLMv2**. The key innovation is that it uses a simpler unified architecture to understand both text and images, instead of having separate components.

It has been "pre-trained" on a huge dataset of millions of document images and text to learn patterns. This pre-training involves masking and predicting parts of text and images to align them.

### Benefits

This unified pre-training allows **LayoutLMv3** to achieve state-of-the-art accuracy on tasks like:

- Form understanding
- Receipt understanding
- Answering questions about documents
- Classifying document types
- Extracting layout and structure

So it provides excellent performance on both text-centric and image-centric document AI problems.

### Why It Matters

The simplicity yet high performance opens up **LayoutLMv3** to be a general-purpose backbone for document-based AI applications. Instead of needing custom models, **LayoutLMv3** provides an off-the-shelf model to kickstart development.

This will help accelerate building solutions that can deeply understand documents involving text, tables, forms, diagrams and more. Please let me know if you have any other questions!



### Applying Just the Traditional OCR is not scalable solution for Perioli

Here is a comparison focusing on why applying only a traditional OCR solution is not scalable for Perioli:

### Not Scalable: Traditional OCR

- Perioli has hundreds of document layouts and templates. An engineer would have to manually inspect each one and configure customised OCR extraction rules.
- There is likely a long tail of rare, unseen documents. Each new variation would require additional OCR programming.
- OCR relies on brittle templates and cannot understand semantic concepts like tables, columns etc. Any nested structures need complex custom logic.
- Traditional OCR only extracts raw text - no understanding of fields, entities etc. Post-OCR logic needs to transform text snippets into structured data.
- Every new document domain would require building OCR templates and post-processing logic from scratch. Existing mapping cannot be generalised.

In summary, the manual effort and customization required for traditional OCR does not scale to Perioli's needs. It would require large engineering resources while still delivering limited accuracy and flexibility.

The solution is an intelligent document analysis approach like LayoutLMv3 instead, which uses deep learning to develop document understanding and generalise across layouts. This automates the process and provides the scalability Perioli requires when dealing with hundreds of highly varied document templates over time.



# Traditional OCR vs Artificial Intelligence (LayoutLMV3)

The shift from conventional Optical Character Recognition (OCR) systems to AI-driven approaches for extracting information from image documents introduces several notable differences that bring about improvements in flexibility, accuracy, and applicability. Below, I will elaborate on the key distinctions between these two paradigms:

## 1. Technology Base

**Traditional OCR:** Depends on rule-based pattern recognition algorithms. These systems thrive on clean, standardized images with minimal background noise, matching shapes of letters and numbers to a predefined set of characters.

*Fig 1: Same rules, different invoices:*

**AI-based OCR:** Employs machine learning (ML) and deep learning (DL) models, including Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). Unlike traditional OCR, AI-based systems learn from extensive datasets, gaining the ability to understand context, style variations, and even handwritten text.

*Fig 2: AI does not require the same set up of rules or templates*

## 2. Accuracy and Error Tolerance

**Traditional OCR:** Less effective at handling variations in text font, size, style, or background noise. Accuracy decreases notably when dealing with poor quality documents or images that deviate from its trained patterns.

**AI-based OCR:** Exhibits higher error tolerance due to learning capabilities. It accurately processes documents with diverse fonts, sizes, styles, backgrounds, and distortions, thanks to exposure to vast and varied datasets during training.

## 3. Learning and Adaptation

**Traditional OCR:** Requires manual reconfiguration and updating of rules and patterns for adaptation to new document types, fonts, or styles.

**AI-based OCR:** Continuously improves and adapts as it processes more data. AI models refine their ability to recognize and interpret text over time, accommodating new document types or writing styles without explicit reprogramming.

## 4. Handling of Unstructured Data

**Traditional OCR:** Struggles with unstructured data such as forms, invoices, or receipts that lack a uniform layout or structure, leading to challenging and often inaccurate information extraction.

**AI-based OCR:** Excels at processing and extracting information from unstructured or semi-structured documents. It understands and interprets context, enabling identification and extraction of specific information from diverse document layouts.

## 5. Language and Handwriting Recognition

**Traditional OCR:** Limited in recognizing multiple languages, especially those with complex alphabets. Handwritten text recognition is generally poor.

**AI-based OCR:** Significantly better at recognizing multiple languages, including those with non-Latin alphabets. It can learn to read and interpret various handwriting styles, though accuracy varies based on the quality of the handwriting and available training data.

## 6. Integration and Scalability

**Traditional OCR:** Integrating and scaling traditional OCR solutions is relatively straightforward due to their simple and fixed functionalities.

**AI-based OCR:** While potentially more complex to integrate initially, AI-based OCR offers greater scalability and flexibility. It can be customized and expanded more easily to meet evolving business needs and handle larger volumes of documents.



### What is OCR (Optical Character Recognition)?

Optical Character Recognition (OCR) is the process that converts an image of text into a machine-readable text format. For example, if you scan a form or a receipt, your computer saves the scan as an image file. You cannot use a text editor to edit, search, or count the words in the image file. However, you can use OCR to convert the image into a text document with its contents stored as text data.

### Why is OCR important?

Most business workflows involve receiving information from print media. Paper forms, invoices, scanned legal documents, and printed contracts are all part of business processes. These large volumes of paperwork take a lot of time and space to store and manage. Though paperless document management is the way to go, scanning the document into an image creates challenges. The process requires manual intervention and can be tedious and slow.

Moreover, digitizing this document content creates image files with the text hidden within it. Text in images cannot be processed by word processing software in the same way as text documents. OCR technology solves the problem by converting text images into text data that can be analyzed by other business software. You can then use the data to conduct analytics, streamline operations, automate processes, and improve productivity.
