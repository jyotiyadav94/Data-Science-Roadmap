# LLAMA2 Fine Tuning

[LLAMA2 Fine Tuning Notebook](https://github.com/jyotiyadav94/Fine-tuning-NER-LLM/blob/main/NER_fine_tuning_llama_2_(1).ipynb)

Language models are artificial intelligence systems that have been trained to understand and generate human language. Large Language Models (LLMs) like GPT-3, ChatGPT, GPT-4, Bard, and similar can perform diverse sets of tasks out of the box. Often the quality of output from these large language models is highly dependent on the finesse of the prompt given by the user.

These Language models are trained on vast amounts of text data from the Internet. Most of the language models are trained in an auto-regressive way i.e. they try to maximize the probability of the next word based on the words they have produced or seen in the past. This data includes a wide range of written text, from books and articles to websites and social media posts. Language models have a wide range of applications, including chatbots, virtual assistants, content generation, and more. They can be used in industries like customer service, healthcare, finance, and marketing.

Since these models are trained on enormous data, they are already good at zero-shot inference and can be steered to perform better with few-shot examples. Zero-shot is a setup in which a model can learn to recognize things that it hasn't explicitly seen before in training. In a Few-shot setting, the goal is to make predictions for new classes based on the few examples of labeled data that are provided to it at inference time. Despite their amazing capabilities of generating text, these humongous models come with a few limitations that must be thought of when building an LLM-based production pipeline. Some of these limitations are hallucinations, biases, and more.

## About LLama2

Meta's open-source LLM is called Llama 2. It was trained with 2 trillion "tokens" from publicly available sources like Wikipedia, Common Crawl, and books from the Gutenberg project. Three different parameter level model versions are available, i.e. 7 billion, 13 billion, and 70 billion parameter models. There are two types of completion models available: Chat-tuned and General. The chat-tuned models that have been fine-tuned for chatbot-like dialogue are denoted by the suffix '-chat'. We will use general Meta's 7b Llama-2 huggingface model as the base model that we fine-tune. Feel free to use any other version of llama2-7b. In this project we use Inputs as words as input_text and labels as output_text.

- Input Models: input text only.
- Output Models: generate text only.
- Model Architecture: Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.

Llama 2 family of models. Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models - 70B -- use Grouped-Query Attention (GQA) for improved inference scalability.

Model Dates: Llama 2 was trained between January 2023 and July 2023.

Status: This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.

## Add PEFT Config

PEFT ðŸ¤— PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a modelâ€™s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware.

PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference.

In the case of LLMs, which can have billions of parameters, changing all of them during training can be computationally expensive and memory-intensive.

PEFT, as a subset of fine-tuning, takes parameter efficiency seriously. Instead of altering all the coefficients of the model, PEFT selects a subset of them, significantly reducing the computational and memory requirements. This approach is particularly useful when training large models, like Falcon 7B, where efficiency is crucial.

## Training, Fine-Tuning, and Prompt Engineering: Key Differences

Before diving deeper into PEFT, letâ€™s clarify the distinctions between training, fine-tuning, and prompt engineering. These terms are often used interchangeably but have specific meanings in the context of LLMs.

- Training: When a model is created from scratch, it undergoes training. This involves adjusting all the modelâ€™s coefficients or weights to learn patterns and relationships in data. Itâ€™s like teaching the model the fundamentals of language.
- Fine-Tuning: Fine-tuning assumes the model already has a basic understanding of language (achieved through training). It involves making targeted adjustments to adapt the model to a specific task or domain. Think of it as refining a well-educated model for a particular job, such as answering questions or generating text.
- Prompt Engineering: Prompt engineering involves crafting input prompts or questions that guide the LLM to provide desired outputs. Itâ€™s about tailoring the way you interact with the model to get the results you want.

PEFT plays a significant role in the fine-tuning phase, where we selectively modify the modelâ€™s coefficients to improve its performance on specific tasks.

## Exploring LoRA and QLoRA for Coefficient Selection

the heart of PEFT and understand how to select the subset of coefficients efficiently. Two techniques, LoRA (Low-Rank Adoption) and QLoRA (Quantization + LoRA), come into play for this purpose.

- LoRA (Low-Rank Adoption): LoRA is a technique that recognizes that not all coefficients in a model are equally important. It exploits the fact that some weights have more significant impacts than others. In LoRA, the large weight matrix is divided into two smaller matrices by factorization. The â€˜Râ€™ factor determines how many coefficients are selected. By choosing a smaller â€˜R,â€™ we reduce the number of coefficients that need adjustment, making the fine-tuning process more efficient.
- Quantization: Quantization involves converting high-precision floating-point coefficients into lower-precision representations, such as 4-bit integers. While this introduces information loss, it significantly reduces memory requirements and computational complexity. When multiplied, these quantized coefficients are dequantized to mitigate the impact of error accumulation.

Imagine an LLM with 32-bit coefficients for every parameter. Now, consider the memory requirements when dealing with billions of parameters. Quantization offers a solution by reducing the precision of these coefficients. For instance, a 32-bit floating-point number can be represented as a 4-bit integer within a specific range. This conversion significantly shrinks the memory footprint.

However, thereâ€™s a trade-off; quantization introduces errors due to the information loss. To mitigate this, dequantization is applied when the coefficients are used in calculations. This balance between memory efficiency and computational accuracy is vital in large models like Falcon 7B.

## Key Takeaways

- PEFT (Parameter Efficient Fine-Tuning) reduces computational and memory demands in large language models by making targeted coefficient adjustments.
- LoRA (Low-Rank Adoption) selects vital coefficients, while quantization reduces memory usage by converting high-precision coefficients into lower-precision forms, both crucial in PEFT.
- Fine-tuning LLMs with PEFT involves structured data preparation, library setup, model selection, PEFT configuration, quantization choices, and vigilant monitoring of training and validation loss to balance efficiency and model performance.

```python
def create_peft_config(m):
    peft_cofig = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=16,
        lora_dropout=0.05,
        target_modules=['q_proj', 'v_proj'],
    )
    model = prepare_model_for_int8_training(m)
    model.enable_input_require_grads()
    model = get_peft_model(model, peft_cofig)
    model.print_trainable_parameters()
    return model, peft_cofig

model, lora_config = create_peft_config(model)
```
