# TorchServe

TorchServe is an open-source model serving framework for PyTorch that simplifies the deployment of trained PyTorch models at scale without the need for custom code.

TorchServe provides lightweight serving with low latency, enabling the deployment of models for high-performance inference.

## Features of TorchServe

1. **Model agnostic**: TorchServe is designed to support any model trained using the PyTorch framework, making it compatible with a wide range of deep learning architectures. It offers a standardized way to package and serve models without requiring modifications to the model architecture or code. Non-PyTorch models (such as scikit-learn, ONNX, etc.) can be served using the CustomHandler.

2. **Easy Deployment**: Deploying a trained model using TorchServe is straightforward. You can define a model archive (.mar files), which includes the serialized model, model handler, and extra files. Each model can accept a custom requirements file to handle dependencies. TorchServe automates the deployment process using a properties file (config.properties), which includes loading the model into memory and exposing it as a REST API endpoint.

3. **API**: TorchServe provides a RESTful API for serving models, allowing for inference using HTTP and gRPC requests. This makes it easy to integrate the model-serving functionality into various applications and programming languages.

In addition...

## Steps for Deploying Models with TorchServe & Docker

1. Install model archiver and build TorchServe image.
2. Select an appropriate default handler (such as image classification) or create a custom handler.
3. Prepare essential files for configuration.
4. Use `torch-model-archiver` to package your model and handler into a .mar file and place it in the model store.
5. Start the container.
6. Run inference.

One file to include in the ".mar" file is the handler, containing information about preprocessing, inference, and postprocessing for predictions. The default handler, `ImageClassifier`, can be used for image classification problems trained on ImageNet. However, for most applications, a custom handler script needs to be defined. TorchServe offers a class called `BaseHandler`, where the steps of preprocessing, inference, and postprocessing are handled for a default setup.

To deploy custom models, create a handler class inheriting from `BaseHandler` and customize the necessary parts.


## TorchServe Handler Methods

### `__init__`

As always in this part, the needed variables are defined. Some important ones are: `model`, `mapping`, and `context`.

### Some important initialized variables in the base handler

Here we might additionally add transformations or whatever we need for our model.

### `initialize`

This method loads the `model.pt` file and sets it to eval-mode. First, it tries to load TorchScript, else it loads the state_dict-based model. As input, it gets the context-json file with all the needed information to load the model, e.g., its location. It raises an error when the `model.py` file is missing. It also sets the `device` and `map_location` variables.

### `preprocess`

In this method, the preprocessing steps are defined, which need to be performed on the data before applying the model. As input, it gets the data as a list, and it returns the preprocessed data as a tensor. These steps need to be customized!

### `inference`

In this method, the model is applied, and the actual predictions are created. It receives the data as returned from the `preprocess` function. This should match the model input shape. The predictions are returned as a tensor.

### `postprocess`

This method receives as input data the predictions returned from the `inference` function. It then creates the actual output of the model as a list. These steps need to be customized.


### `handler.py`

```python
# custom handler file
from typing import Dict, List, Tuple
import numpy as np
import soundfile as sf
from io import BytesIO
from ts.torch_handler.base_handler import BaseHandler

import torch
from model import Model

class SoundModelHandler(BaseHandler):
    def __init__(self):
        super(SoundModelHandler, self).__init__()
        self.initialized = False

    def preproc_one_wav(self, req: Dict[str, bytearray]) -> Tuple[np.ndarray, int]:
        """
        Function to convert req data to image
        :param req:
        :return: np array of wav form
        """
        wav_byte = req.get("data")
        
        if wav_byte is None:
            wav_byte = req.get('body')
        
        # create a stream from the encoded image
        wav, sr = sf.read(BytesIO(wav_byte))
        return wav, sr
        
    def initialize(self, context):
        """
        Invoke by torchserve for loading a model
        :param context: context contains model server system properties
        :return:
        """
        self.manifest = context.manifest
        
        properties = context.system_properties
        model_dir = properties.get("model_dir")
        self._context = context

        self.model = Model()
        self.model.load_state_dict(torch.load(model_dir + 'weights.pt'))
        
        self.initialize = True
        self.device = torch.device(
            "cuda:" + str(properties.get("gpu_id"))
            if torch.cuda.is_available() and properties.get("gpu_id") is not None
            else "cpu"
        )

        
    def preprocess(self, requests: List[Dict[str, bytearray]]):
        """
        Function to prepare data from the model
        
        :param requests:
        :return: tensor of the processed shape specified by the model
        """
        batch_crop = [self.preproc_one_wav(req) for req in requests]

        # Do something here if you want to return as a torch tensor
        # You can apply torch cat here as well
        batch = torch.cat(batch_crop)
        
        return batch
        
    def inference(self, model_input: torch.Tensor):
        """
        Given the data from .preprocess, perform inference using the model.
        
        :param reqeuest:
        :return: Logits or predictions given by the model
        """
        with torch.no_grad():
            generated_ids = self.model.generate(model_input.to(self.device))
        
        return generated_ids
```


```python
    import json
    import requests

    sample = {'data': wav_in_byte}
    results = requests.post('ip:port', data=json.dumps(sample))
    # parse results
    get_results = results.json()

```

Note: Make sure to replace placeholders like 'ip:port' and 'model_dir + 'weights.pt'' with the actual values.
